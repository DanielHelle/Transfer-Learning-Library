Namespace(root='data/office31', data='Office31', source=['A'], target=['W'], train_resizing='res.', val_resizing='res.', resize_size=32, scale=[0.08, 1.0], ratio=[0.75, 1.3333333333333333], no_hflip=False, norm_mean=(0.485, 0.456, 0.406), norm_std=(0.229, 0.224, 0.225), arch='resnet18', bottleneck_dim=256, no_pool=False, scratch=False, trade_off=1.0, batch_size=1, lr=0.01, lr_gamma=0.001, lr_decay=0.75, momentum=0.9, weight_decay=0.001, workers=2, epochs=20, iters_per_epoch=1000, print_freq=100, seed=None, per_class_eval=False, log='logs/dann/Office31_A2W', phase='train', download_dataset_only='False', dataset_condensation='True', condensed_data_path='/home/daniel/exjobb/DatasetCondensation/result/res_DC_pre_processed_office31_ConvNet_1ipc.pt')
train_transform:  None
val_transform:  Compose(
    ResizeImage(size=(32, 32))
    ToTensor()
    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
)
=> using model 'resnet18'
lr: 0.001
PPPPPPPP
tensor([27])
torch.Size([1, 3, 32, 32])
tensor([[[[ 0.1491,  0.9019,  0.9738,  ...,  0.8063,  0.9209,  0.6837],
          [ 1.1112,  2.0744,  1.6479,  ...,  1.6911,  2.0435,  0.5254],
          [ 0.5255,  1.7873,  1.0462,  ...,  0.8157,  1.4250,  1.1898],
          ...,
          [ 0.4108,  1.3978,  1.2222,  ...,  1.0460,  1.6606,  1.0631],
          [ 0.4433,  1.8851,  1.5905,  ...,  1.4465,  1.9811,  0.6724],
          [-0.4033,  0.8221,  0.9801,  ...,  0.7301,  0.7490,  0.2390]],

         [[-0.8765,  1.2794,  0.8526,  ...,  0.9905,  0.9689,  0.2072],
          [ 1.1309,  2.0199,  1.5750,  ...,  1.7619,  2.0478,  0.3206],
          [ 0.3499,  1.8984,  1.0794,  ...,  1.1856,  1.7377,  0.6753],
          ...,
          [ 0.1148,  1.6704,  1.1407,  ...,  1.2932,  1.7499,  0.6623],
          [ 0.3409,  1.7828,  1.7925,  ...,  1.5571,  2.0801,  0.5998],
          [-0.3888,  0.8400,  0.5261,  ...,  0.7581,  1.3294,  0.3071]],

         [[ 0.2451,  1.0702,  0.6693,  ...,  1.0114,  1.0367, -0.4751],
          [ 0.9747,  2.2003,  1.8100,  ...,  1.9592,  1.7191,  0.4964],
          [ 0.5882,  1.9298,  1.1875,  ...,  1.2161,  1.9451,  0.9981],
          ...,
          [ 0.3795,  1.6416,  1.4504,  ...,  1.2796,  1.5584,  0.5659],
          [ 0.3967,  2.1412,  2.1369,  ...,  1.9648,  2.5819,  1.0597],
          [-0.4479,  1.0822,  0.8689,  ...,  1.0955,  0.8784,  0.0833]]]])
Traceback (most recent call last):
  File "/home/daniel/exjobb/Transfer-Learning-Library/examples/domain_adaptation/image_classification/dann.py", line 312, in <module>
    main(args)
  File "/home/daniel/exjobb/Transfer-Learning-Library/examples/domain_adaptation/image_classification/dann.py", line 153, in main
    train(train_source_iter, train_target_iter, classifier, domain_adv, optimizer,
  File "/home/daniel/exjobb/Transfer-Learning-Library/examples/domain_adaptation/image_classification/dann.py", line 201, in train
    x_t, = next(train_target_iter)[:1]
  File "/home/daniel/miniconda3/envs/transfer/lib/python3.9/site-packages/tllib-0.4-py3.9.egg/tllib/utils/data.py", line 50, in __next__
  File "/home/daniel/miniconda3/envs/transfer/lib/python3.9/site-packages/torch-1.7.1-py3.9-linux-x86_64.egg/torch/utils/data/dataloader.py", line 435, in __next__
    data = self._next_data()
  File "/home/daniel/miniconda3/envs/transfer/lib/python3.9/site-packages/torch-1.7.1-py3.9-linux-x86_64.egg/torch/utils/data/dataloader.py", line 1085, in _next_data
    return self._process_data(data)
  File "/home/daniel/miniconda3/envs/transfer/lib/python3.9/site-packages/torch-1.7.1-py3.9-linux-x86_64.egg/torch/utils/data/dataloader.py", line 1111, in _process_data
    data.reraise()
  File "/home/daniel/miniconda3/envs/transfer/lib/python3.9/site-packages/torch-1.7.1-py3.9-linux-x86_64.egg/torch/_utils.py", line 428, in reraise
    raise self.exc_type(msg)
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/daniel/miniconda3/envs/transfer/lib/python3.9/site-packages/torch-1.7.1-py3.9-linux-x86_64.egg/torch/utils/data/_utils/worker.py", line 198, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/daniel/miniconda3/envs/transfer/lib/python3.9/site-packages/torch-1.7.1-py3.9-linux-x86_64.egg/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/home/daniel/miniconda3/envs/transfer/lib/python3.9/site-packages/torch-1.7.1-py3.9-linux-x86_64.egg/torch/utils/data/_utils/collate.py", line 83, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/home/daniel/miniconda3/envs/transfer/lib/python3.9/site-packages/torch-1.7.1-py3.9-linux-x86_64.egg/torch/utils/data/_utils/collate.py", line 83, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/home/daniel/miniconda3/envs/transfer/lib/python3.9/site-packages/torch-1.7.1-py3.9-linux-x86_64.egg/torch/utils/data/_utils/collate.py", line 85, in default_collate
    raise TypeError(default_collate_err_msg_format.format(elem_type))
TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>

