Namespace(root='data/office31', data='Office31', source=['A'], target=['W'], train_resizing='res.', val_resizing='res.', resize_size=32, scale=[0.08, 1.0], ratio=[0.75, 1.3333333333333333], no_hflip=False, norm_mean=(0.485, 0.456, 0.406), norm_std=(0.229, 0.224, 0.225), arch='convnet', bottleneck_dim=256, no_pool=False, scratch=False, trade_off=1.0, batch_size=1, lr=0.01, lr_gamma=0.001, lr_decay=0.75, momentum=0.9, weight_decay=0.001, workers=2, epochs=1000, iters_per_epoch=1000, print_freq=100, seed=None, per_class_eval=False, log='logs/dann/Office31_A2W', phase='train', download_dataset_only='False', dataset_condensation='True', condensed_data_path='/home/daniel/exjobb/DatasetCondensation/result/res_DC_pre_processed_office31_ConvNet_1ipc.pt', no_aug='False', channel=3, convnet_weights_data_path='/home/daniel/exjobb/DatasetCondensation/result/state_dict_DC_pre_processed_office31_ConvNet_1ipc.pt')
train_transform:  Compose(
    ResizeImage(size=(32, 32))
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
)
val_transform:  Compose(
    ResizeImage(size=(32, 32))
    ToTensor()
    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
)
=> using model 'convnet'
convnet
ConvNet(
  (features): Sequential(
    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): GroupNorm(128, 128, eps=1e-05, affine=True)
    (2): ReLU(inplace=True)
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): GroupNorm(128, 128, eps=1e-05, affine=True)
    (6): ReLU(inplace=True)
    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): GroupNorm(128, 128, eps=1e-05, affine=True)
    (10): ReLU(inplace=True)
    (11): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): Linear(in_features=128, out_features=31, bias=True)
)
FFFFFFF
Comparing shapes of ConvNet weights and loaded weights:
Shapes match for features.0.weight
Shapes match for features.0.bias
Shapes match for features.1.weight
Shapes match for features.1.bias
Shapes match for features.4.weight
Shapes match for features.4.bias
Shapes match for features.5.weight
Shapes match for features.5.bias
Shapes match for features.8.weight
Shapes match for features.8.bias
Shapes match for features.9.weight
Shapes match for features.9.bias
Shape mismatch: classifier.weight
  ConvNet shape: torch.Size([31, 128])
  Loaded shape: torch.Size([31, 2048])
Shapes match for classifier.bias
Traceback (most recent call last):
  File "/home/daniel/exjobb/Transfer-Learning-Library/examples/domain_adaptation/image_classification/dann.py", line 320, in <module>
    main(args)
  File "/home/daniel/exjobb/Transfer-Learning-Library/examples/domain_adaptation/image_classification/dann.py", line 108, in main
    backbone = utils.get_model(args.arch, pretrain=not args.scratch, channel=args.channel,num_classes=num_classes,args = args)
  File "/home/daniel/exjobb/Transfer-Learning-Library/examples/domain_adaptation/image_classification/utils.py", line 103, in get_model
    backbone.load_state_dict(weights)
  File "/home/daniel/miniconda3/envs/transfer/lib/python3.9/site-packages/torch-1.7.1-py3.9-linux-x86_64.egg/torch/nn/modules/module.py", line 1051, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ConvNet:
	size mismatch for classifier.weight: copying a param with shape torch.Size([31, 2048]) from checkpoint, the shape in current model is torch.Size([31, 128]).
